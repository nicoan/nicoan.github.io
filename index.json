[{"content":"A while ago, I published an app that I originally made for myself but thought it would be useful for others: Kindly RSS Reader. Kindly RSS Reader is a self-hosted RSS aggregator designed for e-ink devices and optimized for low-end computers such as the Raspberry Pi. In fact, I run it on a Raspberry Pi 3B powered by a USB port.\nThe only way to deploy it at the moment is via Docker (or compiling the source and running it by manually). I’ve uploaded the image in Docker Hub. In the future, I plan to create a .deb package and other formats to make deployment more flexible.\nIssues requesting support for other architectures I was pleasantly surprised by the attention the project received. After a week of publishing the repository I got received two issues asking to support older arm architectures and for x86_64.\nSupporting more architectures was something I had in mind from the beginning of the project. There are a lot of single-board and low end computers out there that could run the project, so I was happy to see people interested in running it on different architectures.\nThe approach At the moment I got two computers: A MacBook running an ARMv8 and an old desktop running x86_64. I wanted the build process to be as portable as possible, so I could use either machine, and I usually do not like to install things in the OS unless is absolutely necessary. Using docker for building met both my requirements.\nThe final images I wanted the final images to be as lightweight as possible. That’s why I opted for Alpine Linux docker images. Alpine Linux is a small1 linux distribution focused on security.\nOne thing that differentiates Alpine from other distributions is that it uses musl C library instad of GNU C Library. This is an important detail for cross-compilation.\nThe linkers image To be able to cross compile projects, we need to use different linkers: one for each target architecture. The target architectures are armv6, armv7, arm64v8 (or aarch64) and x86_64.\nThe first step was to create a Docker image containing all the necessary linkers. To download and install the them I used the musl-cross-make project.\n# This image will be used for building the project in different platforms FROM rust:1.84-bullseye AS builder WORKDIR /home RUN git clone https://github.com/richfelker/musl-cross-make.git --depth 1 # armv6 RUN cd musl-cross-make \\ \u0026amp;\u0026amp; echo \u0026#39;TARGET = arm-linux-musleabihf\u0026#39; \u0026gt; config.mak \\ \u0026amp;\u0026amp; echo \u0026#39;OUTPUT = /build/cross-armv6\u0026#39; \u0026gt;\u0026gt; config.mak \\ \u0026amp;\u0026amp; make \\ \u0026amp;\u0026amp; make install # armv7 RUN cd musl-cross-make \\ \u0026amp;\u0026amp; echo \u0026#39;TARGET = armv7-linux-musleabihf\u0026#39; \u0026gt; config.mak \\ \u0026amp;\u0026amp; echo \u0026#39;OUTPUT = /build/cross-armv7\u0026#39; \u0026gt;\u0026gt; config.mak \\ \u0026amp;\u0026amp; make \\ \u0026amp;\u0026amp; make install # arm64v8 RUN cd musl-cross-make \\ \u0026amp;\u0026amp; echo \u0026#39;TARGET = aarch64-linux-musl\u0026#39; \u0026gt; config.mak \\ \u0026amp;\u0026amp; echo \u0026#39;OUTPUT = /build/cross-armv8\u0026#39; \u0026gt;\u0026gt; config.mak \\ \u0026amp;\u0026amp; make \\ \u0026amp;\u0026amp; make install # x86_64 RUN cd musl-cross-make \\ \u0026amp;\u0026amp; echo \u0026#39;TARGET = x86_64-linux-musl\u0026#39; \u0026gt; config.mak \\ \u0026amp;\u0026amp; echo \u0026#39;OUTPUT = /build/cross-x86_64\u0026#39; \u0026gt;\u0026gt; config.mak \\ \u0026amp;\u0026amp; make \\ \u0026amp;\u0026amp; make install ENTRYPOINT [\u0026#34;/bin/bash\u0026#34;] As base image, I used rust:1.84-bullseye because it includes all the tools I need for building the project (with the exception of the additional targets, which will be installed manually in the respective Dockerfile).\nThe command used for building this image is:\ndocker build \\ --tag nicoan/kindly-rss-builder \\ -f ./dockerfiles/Dockerfile.build \\ . Building the images To build the project, I use four different Dockerfiles: One for each architecture. I could have created one Dockerfile with parameters but I wanted to take advantage of the Docker’s layer caching mechanism. Here’s the Dockerfile for the armv6 architecture:\nFROM --platform=$BUILDPLATFORM nicoan/kindly-rss-builder AS builder WORKDIR /home ENV PATH=$PATH:/build/cross-armv6/bin ENV CARGO_TARGET_ARM_UNKNOWN_LINUX_MUSLEABIHF_LINKER=arm-linux-musleabihf-gcc RUN rustup target add arm-unknown-linux-musleabihf COPY . ./ RUN cargo build --target arm-unknown-linux-musleabihf --release FROM alpine:3 AS run RUN mkdir -p /home/kindlyrss/static_data \\ \u0026amp;\u0026amp; mkdir -p /home/kindlyrss/data EXPOSE 3000/tcp COPY --from=builder /home/target/arm-unknown-linux-musleabihf/release/kindle-rss-reader /usr/local/bin/kindlyrss COPY --from=builder /home/templates/ /home/kindlyrss/static_data/templates/ COPY --from=builder /home/migrations/ /home/kindlyrss/static_data/migrations/ COPY --from=builder /home/static/ /home/kindlyrss/static_data/static/ COPY --from=builder /home/config/config.json /home/kindlyrss/data/config.json ENV RUST_LOG=info ENV MAX_ARTICLES_QTY_TO_DOWNLOAD=0 ENV STATIC_DATA_PATH=/home/kindlyrss/static_data ENV DATA_PATH=/home/kindlyrss/data CMD [\u0026#34;kindlyrss\u0026#34;] The idea is to use a multi-stage build with two stages: build and run.\nIn the build stage:\nLine 1: We use the linkers image with the --platform=$BUILDPLATFORM parameter to ensure that no emulation will be used in this stage. Line 5: We add to the $PATH environment variable the path where the linker’s binaries are located. Line 6: We tell cargo which linker we are going to use for compiling armv6. This can be done through environment variables or with a configuration file. The shape of the environment variables specifying the linker is CARGO_TARGET_\u0026lt;triple\u0026gt;_LINKER. The format of the \u0026lt;triple\u0026gt; is \u0026lt;arch\u0026gt;\u0026lt;sub\u0026gt;-\u0026lt;vendor\u0026gt;-\u0026lt;sys\u0026gt;-\u0026lt;abi\u0026gt;. The four environment variables used are: CARGO_TARGET_ARM_UNKNOWN_LINUX_MUSLEABIHF_LINKER for armv6 CARGO_TARGET_ARMV7_UNKNOWN_LINUX_MUSLEABIHF_LINKER for armv7 CARGO_TARGET_AARCH64_UNKNOWN_LINUX_MUSL_LINKER for arm64v8 CARGO_TARGET_X86_64_UNKNOWN_LINUX_MUSL_LINKER for x86_64 Line 8: We add the arm-unknown-linux-musleabihf target to be able to build the project for the armv6 architecture using the musl C library. The four targets used are: arm-unknown-linux-musleabihf for armv6 armv7-unknown-linux-musleabihf for armv7 aarch64-unknown-linux-musl for arm64v8 x86_64-unknown-linux-musl for x86_64 Line 9: We copy the source. Line 11: We build the project in release mode targeting the armv6 architecture. The run stage will be built using alpine as base. This stage is the last one and will output the final image for armv6. Here, we copy the binary file and all the files needed by it to run the application correctly from the builder stage, expose the port 3000 and set some configurations using environment variables. After that we execute the binary to run the application.\nNote that in the Dockerfile is never specified the platform to be used. This is done using the --platform argument in the docker build command.\nThe command used to build this image is:\ndocker build \\ --tag nicoan/kindly-rss-reader:$(PACKAGE_VERSION)-armv6 \\ --platform linux/arm/v6 \\ -f ./dockerfiles/Dockerfile.armv6 \\ . where $(PACKAGE_VERSION) is the app version.\nPublishing the images To publish the images we just need a Docker Hub account and use the docker push command. Since we are supporting more than one architecture, it would be nice to have all the images targeting different platforms grouped in the same tag. This can be achieved by pushing all the different images separately and then creating a manifest pointing at them under the same tag.\nFor example, for version 0.1.0, we first push the four images:\ndocker push nicoan/kindly-rss-reader:0.1.0-x86_64 docker push nicoan/kindly-rss-reader:0.1.0-arm64v8 docker push nicoan/kindly-rss-reader:0.1.0-armv7 docker push nicoan/kindly-rss-reader:0.1.0-armv6 After that we create a manifest file for the tag 0.1.0 attaching all the images that we just pushed to the hub:\ndocker manifest create nicoan/kindly-rss-reader:0.1.0 \\ --amend nicoan/kindly-rss-reader:0.1.0-x86_64 \\ --amend nicoan/kindly-rss-reader:0.1.0-arm64v8 \\ --amend nicoan/kindly-rss-reader:0.1.0-armv7 \\ --amend nicoan/kindly-rss-reader:0.1.0-armv6 And push it:\ndocker manifest push nicoan/kindly-rss-reader:0.1.0 The same thing can be done with the latest tag with an extra step: we first need to delete its manifest and then re-create it with the images we want to group. If we don’t do the delete step, the new images will be grouped with all the previous ones.\nDoing all this process semi-automatically I am not good at remembering all the arguments and details of the commands I use. That’s why I usually try to create scripts to automate the processes. In this case I opted to use a good old Makefile:\n# Extract the version from Cargo.toml PACKAGE_VERSION=$(shell cat Cargo.toml | grep version | head -n 1 | awk \u0026#39;{print $$3}\u0026#39; | sed -e \u0026#39;s/\u0026#34;//g\u0026#39;) # Build for different archs # I opted to use multiple Dockerfiles to take advantage of the layer caching mechanism. docker-build: docker build \\ --tag nicoan/kindly-rss-reader:$(PACKAGE_VERSION)-x86_64 \\ -f ./dockerfiles/Dockerfile.x86_64 \\ --platform linux/amd64 \\ . docker build \\ --tag nicoan/kindly-rss-reader:$(PACKAGE_VERSION)-arm64v8 \\ -f ./dockerfiles/Dockerfile.armv8 \\ --platform linux/arm64/v8 \\ . docker build \\ --tag nicoan/kindly-rss-reader:$(PACKAGE_VERSION)-armv6 \\ --platform linux/arm/v6 \\ -f ./dockerfiles/Dockerfile.armv6 \\ . docker build \\ --tag nicoan/kindly-rss-reader:$(PACKAGE_VERSION)-armv7 \\ --platform linux/arm/v7 \\ -f ./dockerfiles/Dockerfile.armv7 \\ . # Builds an image with different linkers to be able to build # for different architectures docker-prepare-build-image: docker build \\ --tag nicoan/kindly-rss-builder \\ -f ./dockerfiles/Dockerfile.build \\ . # Push new versions docker-push: # Push different architecture images docker push nicoan/kindly-rss-reader:$(PACKAGE_VERSION)-x86_64 docker push nicoan/kindly-rss-reader:$(PACKAGE_VERSION)-arm64v8 docker push nicoan/kindly-rss-reader:$(PACKAGE_VERSION)-armv7 docker push nicoan/kindly-rss-reader:$(PACKAGE_VERSION)-armv6 # Create manifest for the package version and push docker manifest create nicoan/kindly-rss-reader:$(PACKAGE_VERSION) \\ --amend nicoan/kindly-rss-reader:$(PACKAGE_VERSION)-x86_64 \\ --amend nicoan/kindly-rss-reader:$(PACKAGE_VERSION)-arm64v8 \\ --amend nicoan/kindly-rss-reader:$(PACKAGE_VERSION)-armv7 \\ --amend nicoan/kindly-rss-reader:$(PACKAGE_VERSION)-armv6 docker manifest push nicoan/kindly-rss-reader:$(PACKAGE_VERSION) # Create manifest for the latest tag and push docker manifest rm nicoan/kindly-rss-reader:latest docker manifest create nicoan/kindly-rss-reader:latest \\ --amend nicoan/kindly-rss-reader:$(PACKAGE_VERSION)-x86_64 \\ --amend nicoan/kindly-rss-reader:$(PACKAGE_VERSION)-arm64v8 \\ --amend nicoan/kindly-rss-reader:$(PACKAGE_VERSION)-armv7 \\ --amend nicoan/kindly-rss-reader:$(PACKAGE_VERSION)-armv6 docker manifest push nicoan/kindly-rss-reader:latest git-tag-and-push: git tag v$(PACKAGE_VERSION) git push origin v$(PACKAGE_VERSION) .PHONY: build-docker docker-push docker-prepare-build-image git-tag-and-push This way, I just have to write in the terminal\nmake docker-prepare-build-image make docker-build make docker-push Conclusion Using Docker to cross-compile Rust projects ensures portability and ease of deployment. This process could be used in some CI/CD pipeline fully automate the process.\nResources https://docs.docker.com/build/building/multi-platform/ https://doc.rust-lang.org/cargo/reference/config.html https://www.docker.com/blog/multi-arch-build-and-images-the-simple-way/ Compressed docker images are around 3.5 MBs while Debian stable slim are around 25MBs.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://nicoan.net/posts/cross_compiling_using_docker/","summary":"\u003cp\u003eA while ago, I published an app that I originally made for myself but thought it would be useful for others: \u003ca href=\"https://kindlyrss.app/\"\u003eKindly RSS Reader\u003c/a\u003e. Kindly RSS Reader is a self-hosted RSS aggregator designed for e-ink devices and optimized for low-end computers such as the Raspberry Pi. In fact, I run it on a Raspberry Pi 3B powered by a USB port.\u003c/p\u003e\n\u003cp\u003eThe only way to deploy it at the moment is via Docker (or compiling the source and running it by manually). I’ve uploaded the image in \u003ca href=\"https://hub.docker.com/r/nicoan/kindly-rss-reader/\"\u003eDocker Hub\u003c/a\u003e. In the future, I plan to create a \u003ccode\u003e.deb\u003c/code\u003e package and other formats to make deployment more flexible.\u003c/p\u003e","title":"Cross compiling Rust using Docker"},{"content":"Having a language that does a lot of checks at compile time is not free, it will impact compilation times. Luckily there are some things we can do to speed things up: Dynamic linking, be careful with code generation and caching dependencies.\nDynamic linking is a somewhat difficult thing to achieve in Rust but not impossible. The main reason is that at the time of writing this (Sept-Oct 2024) Rust does not have its own stable ABI and it must rely on the C binary representation (if we want to inter-operate with other languages or other Rust versions). This has some interesting consequences that we will explore in this post.\nCode generation is when the high level representation of the source code is turned into binary code that can be executed by the machine. Given that the rust compiler uses LLVM, the level of optimizations and the quantity of generated code will affect the compilation speed.\nWhat is Linking? Programs are usually divided into several modules and they have numerous dependencies. Linking is a compilation stage where all the compiled code of those modules needed by a program (and the code of the program itself) is made available in the final executable. We have two ways of linking a program: static and dynamic.\nStatic Linking All the code needed by a program (from external modules and the program itself) is put together in the final executable. This creates fat binaries but it makes the program portable.\nDynamic Linking This type of linking must be supported by the operating system (most, if not all of the major operating systems support this). In this approach, instead of containing all the code needed, the executable contains undefined symbols and a list of objects that contain the code for those symbols.\nThese objects, often referred to as libraries, are binary files used to share binary code between several programs. In Microsoft Windows those files are known as DLLs (dynamically-linked library) and in Unix operating systems (Linux, Mac OS, etc) are known as SOs (shred objects).\nWhen running a dynamically linked executable, the operating system loads the program code along with the libraries in memory and do the final linking.\nThis approach creates “thin” binaries and saves disk and memory space, since the code from the libraries are shared among several applications.\nDifferent linking modes in Rust There are several different linking modes that we can use, producing different kind of shared objects, but in this post we will focus only on one of them: dylib.\nIf we put this configuration in our library crates, the Rust compiler will generate a dynamic library that will be dynamically linked with our executable. Isn’t that what we need? Why do we have other configurations?1 Given that Rust does not have an stable ABI yet, there are no guarantees that the compiled libraries will work if we don’t compile the project with the same Rust version used for compiling the library.\nThis mode is suitable for a project where we have one or more library crates that are used by several binary crates (or other libraries). We will usually compile everything altogether the first time and then recompile only the things we change. If we change the Rust version, we need to recompile everything.\nCompilation stages To generate binary objects, Rust compiler must go through several different stages. I am not going to explain how a compiler works in detail, but having a general idea of what happens will help us identify places where we can work to optimize compile times.\nTo measure compilation times, we will use the built-in cargo tool called timings. This tool will generate a detailed HTML report showing how long every compilation unit takes to compile.\nFrom source code to intermediate representation In the timings report, the stages described below are pictured in light blue in the Grantt chart.\nLexing and parsing The compiler first performs the lexing and parsing stage, where lexing is transforming the source code into an stream of tokens and parsing is generating an AST (Abstract Syntax Tree) from those tokens.\nMacro expansion (generating valid Rust code from the macros) is also done at this stage.\nAST Lowering After the AST is created, it is converted into a High Level Intermediate Representation (HIR), this stage is called AST Lowering. In HIR the compiler does type inference, type checking and resolve traits.\nMIR Lowering When HIR is ready, then we enter the MIR lowering stage, that is, transforming HIR to Middle Level Representation (MIR). In this stage, the famous borrow checking is done, code monomorphization, and some optimizations that will improve code generation and compilation speed in that stage.\nCode generation This stage is pictured in purple in the Grantt chart. When we are here, the compiler already has everything represented in MIR. During this phase, the MIR is transformed into LLVM-IR (LLVM Intermediate Representation) and handled to LLVM.\nLLVM does a lot more of optimizations and generates the assembler and binary code that later is linked into the final object.\nIf you want to learn more about the compilation stages, check this article.\nReducing compile times The toy project In order to show how to optimize compiling times, we are going to use a toy project that consist in one library crate and 40 separate binaries that use the library. You may ask yourself: What kind of project has that structure?! It could be a server-less project containing several cloud functions (like AWS Lambdas) that share functionality through some library crates or some project that consist in several binaries (like GNU Core utilities).\nHow I get the timings The timings found on this posts are the last timing returned by cargo after running 10 times the same compilation, always doing cargo clean before executing cargo build [...]. I compile the project 10 times to verify that on average the compilation times are more or less the same.\nInitial compile times Here are the individual compile times for the toy project. This and further compilations were done in Debian 12, with an i7-6700K and 16GB DDR4 2600Mhz Ram 2:\nThe total time was\nFinished `release` profile [optimized] target(s) in 25.30s And the size of the binaries is about 3.5MB:\nls -l --block-size=KB ./target/release total 143262kB -rwxr-xr-x 2 nico nico 3489kB Sep 16 15:03 bin1 -rwxr-xr-x 2 nico nico 3489kB Sep 16 15:03 bin10 -rw-r--r-- 1 nico nico 1kB Sep 16 15:04 bin10.d -rwxr-xr-x 2 nico nico 3489kB Sep 16 15:03 bin11 -rw-r--r-- 1 nico nico 1kB Sep 16 15:04 bin11.d -rwxr-xr-x 2 nico nico 3489kB Sep 16 15:03 bin12 -rw-r--r-- 1 nico nico 1kB Sep 16 15:04 bin12.d -rwxr-xr-x 2 nico nico 3489kB Sep 16 15:03 bin13 The command used was cargo build --release --timings. You can check the source code of the toy project here.\nRemoving unnecessary dependencies It is common that in a project, the people involved usually forget about removing old dependencies. This happens because when projects are large, it is hard to know if a dependency is not used anymore. Luckily, we can use the -Wunused-crate-dependencies flag that tells us which dependencies are not being used by the crates inside the project. If we compile with RUSTFLAGS=-Wunused-crate-dependencies cargo build --release --timings we get the following output:\n... warning: external crate `actix` unused in `lib1`: remove the dependency or add `use actix as _;` | = note: requested on the command line with `-W unused-crate-dependencies` warning: external crate `serde_json` unused in `lib1`: remove the dependency or add `use serde_json as _;` warning: external crate `tokio` unused in `lib1`: remove the dependency or add `use tokio as _;` warning: `lib1` (lib) generated 3 warnings By removing the unused dependencies reported by the warnings, we reduced the total compilation time a little:\nFinished `release` profile [optimized] target(s) in 23.85s It is not much, but by not compiling those dependencies, we gained around 1.x seconds! To keep our project clean, we can use this flag in our CI/CD pipeline to warn us when we forget to remove an old dependency.\nYou can find the modifications made in this section here.\nRemoving unnecessary derives Macros create valid Rust code that then has to be parsed, transformed, validated and optimized. It may happen that you need to derive some trait, not because is used by productive code but is used by test code. It does not make sense to process that code in release builds.\nA nice “trick” to avoid processing that code in release builds is to derive it behind a cargo feature and only activate that feature in the [dev-dependencies] section of the Cargo.toml. The Cargo.toml from lib1 was changed this way:\n[package] name = \u0026#34;lib1\u0026#34; version = \u0026#34;0.1.0\u0026#34; edition = \u0026#34;2021\u0026#34; [dependencies] mockall = { workspace = true, optional = true } reqwest = { workspace = true } serde = { workspace = true } [features] tests = [\u0026#34;dep:mockall\u0026#34;] And we put behind the tests feature, all the code we do not need in production 3:\n#[cfg_attr(feature = \u0026#34;tests\u0026#34;, mockall::automock)] pub trait Trait1 { fn fn1(); fn fn2(a: u16) -\u0026gt; String; fn fn3(a: String) -\u0026gt; u16; } ... #[derive(Serialize, Default)] #[cfg_attr(feature = \u0026#34;tests\u0026#34;, derive(Deserialize, PartialEq, Eq, Debug))] pub struct Struct1 { pub f1: u8, pub f2: String, pub f3: HashMap\u0026lt;String, String\u0026gt;, pub f4: HashSet\u0026lt;String\u0026gt;, pub f5: Vec\u0026lt;String\u0026gt;, } ... #[derive(Deserialize, Default)] #[cfg_attr(feature = \u0026#34;tests\u0026#34;, derive(Serialize, PartialEq, Eq, Debug))] pub struct Struct9 { pub f1: u8, pub f2: String, pub f3: HashMap\u0026lt;String, String\u0026gt;, pub f4: HashSet\u0026lt;String\u0026gt;, pub f5: Vec\u0026lt;String\u0026gt;, } Activating the feature will work for both unit and integration tests. Here are the compilation times after introducing the flag:\nAnd the total time:\nFinished release profile [optimized] target(s) in 21.58s By reducing the code generated by the derives and automock macros, Rust have less code to translate to intermediate representation (light blue), code to generate (purple) and optimize. The time reduction was huge, from an average of 4.x seconds to an average of 0.3 seconds.\nThe take-home lesson here is: do not take the auto generation of code. If you don’t need it in the production build, do not compile it.\nYou can find the modifications made in this section here.\nDynamic Linking Until now, the code contained in lib1 is statically linked to all the binaries in our project. Instead of repeating the code in every binary, we can use dynamic linking to have lib1 as a shared object, allowing the binaries to use the code without the need of having it embedded.\nWith dynamic linking we will not only achieve faster compile times, we will also get smaller binaries and, if there’s a bug in the library, we can fix it and deploy the shared object without the need of modifying the binaries (as long as we use the same Rust version used to compile the binaries).\nTo activate dynamic linking, we need to add to the lib1\u0026rsquo;s Cargo.toml the following lines at the end:\n[lib] crate-type = [\u0026#34;dylib\u0026#34;] And compile ithe project t with: RUSTFLAGS=\u0026quot;-C prefer-dynamic\u0026quot; cargo build --release --timings. Here are the compilation times with dynamic linking:\nThe compilation times for lib1 increased 4, but for binaries times reduced from an average of 0.8x to an average of 0.3x! They were also reduced in size: from 3489 kB to 12 kB!\nls -l --block-size=KB ./target/release total 7238kB -rwxr-xr-x 2 nico nico 12kB Sep 16 14:43 bin1 -rwxr-xr-x 2 nico nico 12kB Sep 16 14:43 bin10 -rw-r--r-- 1 nico nico 1kB Sep 16 14:43 bin10.d -rwxr-xr-x 2 nico nico 12kB Sep 16 14:43 bin11 -rw-r--r-- 1 nico nico 1kB Sep 16 14:43 bin11.d -rwxr-xr-x 2 nico nico 12kB Sep 16 14:43 bin12 ... drwxr-xr-x 2 nico nico 5kB Sep 16 14:43 incremental -rw-r--r-- 1 nico nico 1kB Sep 16 14:43 liblib1.d -rwxr-xr-x 2 nico nico 6555kB Sep 16 14:43 liblib1.so The total time was\nFinished `release` profile [optimized] target(s) in 19.74s If we check the libraries needed for any of our binaries, we are going to see a dependency with liblib1.so. ldd outputs \u0026ldquo;not found\u0026rdquo; because the shared object is located in the target directory at the moment of running the command and not in the usual paths where shared objects can be found (/lib, /usr/lib , /usr/local/lib) or in any of the paths listed in the LD_LIBRARY_PATH environment variable.\n$ ldd ./target/release/bin1 linux-vdso.so.1 (0x00007ffe573aa000) liblib1.so =\u0026gt; not found libstd-52417a9a08ba8fb9.so =\u0026gt; not found libgcc_s.so.1 =\u0026gt; /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fc19594a000) libc.so.6 =\u0026gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007fc195769000) /lib64/ld-linux-x86-64.so.2 (0x00007fc195986000) You can find the modifications made in this section here.\nCache dependencies Most of the total compilation time was taken by the the project\u0026rsquo;s dependencies. In this section we are going to explore two ways those dependencies can be cached so we avoid recompiling them every time we build it. This is specially useful in a continuous integration/deployment environment, where we are constantly compiling the project but the dependencies rarely change.\nsccache sccache is a tool developed by Mozilla. It can be used with several compilers, not only rustc. It works as a wrapper of the compiler, chaching compiled things locally on disk and avoiding recompiling them if possible.\nTo install it, we can run:\n$ cargo install sccache Then, we can use it by wrapping the rustc compiler with the RUSTC_WRAPPER environment variable:\n$ RUSTC_WRAPPER=sccache RUSTFLAGS=\u0026#34;-C prefer-dynamic\u0026#34; cargo build --release --timings We compiled the project with the dynamic linking activated. The first compilation took around 23 seconds, 3.x seconds more than last compilation but, in the first one, sccache was caching the compiled dependencies. After running cargo clean and recompiling the project again we get:\nFinished `release` profile [optimized] target(s) in 6.54s So, we dropped from an average of 19.x seconds to an average of 6.5x seconds!\nCargo Chef cargo-chef is an awesome tool created by Luca Palmieri. It is designed to speed up compilation times when using containers to build the project. Basically, what it does under the hood is locate all the entry points of our workspace either for libs (lib.rs) or binaries (main.rs), remove all the code from them, leave some trivial code like\n// main.rs fn main() {} and compile the project. In other words, it avoids compiling the source code from the project. It just compiles the dependencies to cache them. In future compilations, the dependencies will be already cached, so only the project’s business logic will be compiled.\nAs stated in the official documentation and in a warning if you try to use it locally, this is designed to be used with containers because it leverages on the Docker\u0026rsquo;s layer cache mechanism to work. It is not recommended using it to compile the project locally.\nFor demonstration purposes, I modified the Dockerfile suggested in the official cargo chef documentation:\nFROM lukemathwalker/cargo-chef:latest-rust-1 AS chef WORKDIR /app FROM chef AS planner COPY . . RUN cargo chef prepare --recipe-path recipe.json FROM chef AS builder COPY --from=planner /app/recipe.json recipe.json # Build dependencies - this is the caching Docker layer! RUN CARGO_TARGET_DIR=/app/cache RUSTFLAGS=\u0026#34;-C prefer-dynamic\u0026#34; cargo chef cook --release --workspace --recipe-path recipe.json COPY . . ENTRYPOINT [\u0026#34;/bin/sh\u0026#34;] The image produced from this Dockerfile will contain all the project dependencies already cached in /app/cache directory. It is very important to use cargo chef with exactly the same configuration you are going to use to compile the project. Since we are using the dynamic linking branch for the demonstration, we must include the RUSTFLAGS=\u0026quot;-C prefer-dynamic\u0026quot; flag.\nHere are the steps I followed:\nBuild the image: docker build --tag chef . Enter the container: docker run -it chef Compile the project: CARGO_TARGET_DIR=/app/cache RUSTFLAGS=\u0026quot;-C prefer-dynamic\u0026quot; cargo build --release --workspace. The total compilation time is:\nFinished `release` profile [optimized] target(s) in 2.64s We’ve just compiled the whole project in 2.64s! This is a massive time reduction!\nSummary We started our compile time reduction journey with static linked binaries with a size of 3489kB and a total compilation time of 25.x seconds and we finished it with dynamically linked binaries with a size of 12kB and a total compilation time of 2.x seconds:\nModification Total time % Time reduction from original Original codebase 25.3s 0% Remove unused dependencies 23.85s 5.73% Remove unnecesary derives 21.58s 14.70% Dynamic Linking 19.74s 21.98% Dynamic Linking + sccache 6.54s 75.15% Dynamic Linking + cargo chef 2.64s 89.57% It is important to remember that all the steps include the modifications from the previous steps, with the exception of the caches, that use the dynamic linking branch but sccache and cargo chef are used separately.\nConclusion Sometimes when we are working on projects, deadlines are tight, product team need to release new features and we need to choose wisely on what we spend our time. If we are lucky enough to be in a team that saves time to work on technical debt, we should really use that oportunity to make the structural changes needed in the project to reduce the compilation times. This may sound obvious but not everyone agrees on what is important to solve first.\nWhen projects are small, compilation times are usually small or tolerable, so we don’t pay much attention. As it grows, compilation times may become a real bottleneck for development (imagine that deploying a new version to a dev environment takes an hour).\nTaking care of the compilation will save the whole team a lot of time and headaches, enabling everyone to develop, test and deploy faster.\nResources Learn how to setup dynamically loadable plugins for your Rust app Rust’s official Linkage page Linking Rust crates series Minimizing Compile Times Speeding up incremental Rust compilation with dylibs Build cache There’s a mode that we can use to avoid recompiling the library to match the Rust version we are currently using: cdylib . This mode will produce a dynamic linked library that can be used by other programming languages (and of course, also by Rust). The code compiled with this configuration will follow the C ABI (ordering, size, alignment of fields, etc…) enabling the possibility of directly linking the shared library with a C/C++ program or creating the bindings to use it in another language. The problem with this configuration and Rust is that using the shared object is not straightforward thanks to the C ABI. In another article I will explore this way and show how you can use a Rust library in other languages.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCompilations are quite fast because there\u0026rsquo;s not much code. It is enough to show the compilation times improvements.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nYou may ask yourself why I used a feature flag and #[cfg_attr(feature = \u0026quot;tests\u0026quot;, ...)] instead of plain #[cfg(test)]. With #[cfg(test)], only the current crate would be able to see things under that configuration, or in other words, we would not be able to use the things behind that configuration in the unit and integration tests of the binaries.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI am not sure why the codegen section (purple) disappeared from the graph and why it took almost the double to compile it. I made the modifications described here in some real world projects and the timings certainly did not doubled for the library crates.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://nicoan.net/posts/accelerating_compile_times/","summary":"\u003cp\u003eHaving a language that does a lot of checks at compile time is not free, it will impact compilation times. Luckily there are some things we can do to speed things up: Dynamic linking, be careful with code generation and caching dependencies.\u003c/p\u003e\n\u003cp\u003eDynamic linking is a somewhat difficult thing to achieve in Rust but not impossible. The main reason is that at the time of writing this (Sept-Oct 2024) Rust does not have its own \u003ca href=\"https://en.wikipedia.org/wiki/Application_binary_interface\"\u003estable ABI\u003c/a\u003e and it must rely on the \u003ca href=\"https://doc.rust-lang.org/nomicon/other-reprs.html#reprc\"\u003eC binary representation\u003c/a\u003e (if we want to inter-operate with other languages or \u003cem\u003eother Rust versions\u003c/em\u003e). This has some interesting consequences that we will explore in this post.\u003c/p\u003e","title":"Accelerating Rust compilation times: Dynamic linking, code generation and cache"},{"content":"The bitcoin network has been running non-stop since 2009 and at the time of writing this blog post, the whole blockchain occupies more than 500GB. Verifying information in that huge amount of data is not an easy task. Luckily, as many other computer science problems, this can be solved by using the correct data structure. In our case, that data structure is the Merkle tree.\nWhat is a Merkle Tree (a.k.a Hash Tree)? As the name suggests, the Merkle tree is a data structure in form of a tree (usually binary) where:\nEach leaf contains a hashed piece of information we want to save and later on verify. Each internal node contains the hash of its two children. What and where are they used for? The Merkle Tree usage transcends the blockchain space:\nGit version control system: Git stores everything as a blob (for files) or a tree (for directories, that are also Merkle trees), creating a Merkle tree of the whole project directory. Each blob and tree is part of the outermost tree, meaning that they have assigned a unique hash.\nBy representing all the project structure in a Merkle tree, Git is able to quickly evaluate which files were changed by comparing hashes and, with this information, make diffs, pull and push operations more efficient by only performing them on the files that changed.\nAWS DynamoDB: A high-performance NoSQL key-value store developed by Amazon, it uses Merkle trees to detect inconsistencies between replicas and minimize the amount of transferred data.\nBitTorrent protocol: BitTorrent is a famous peer-to-peer file sharing protocol that uses Merkle Trees for verifying the integrity of the data being downloaded.\nThe data is divided into chunks of equal size and then each chunk is hashed. After that a Merkle tree is calculated from the data hashes and the root is included in the torrent file.\nIntegrity of data pieces can be verified by calculating their hash and constructing the merkle path all the way up to the root. If the constructed root is different from the original contained in the torrent file, then we can be sure that that piece of data is corrupted and the protocol can efficiently download that chunk again.\nThis is still a draft proposal.\nIn this article we will focus on the Blockchain use cases.\nHow is it used in Blockchain? In the Bitcoin whitepaper, Merkle trees are used in several places to solve different problems. Before talking about how Merkle trees are used in Bitcoin, first we need to know how a Bitcoin’s block is composed\nA block contains a list of all the transactions included in that block.Transactions in a Bitcoin’s block are sorted in an specific order and using their transaction id, a Merkle tree from all the transactions contained in that block is generated. It also contains a header with some useful information about the block itself such as block version, transaction’s Merkle tree root, etc. You can check all the fields here.\nBlocks are uniquely identified through their block hash. The block hash is nothing else than the block header hashed twice with the SHA256 hash function. It is important to remember that the previous block hash is included in the block header and that is what creates the chain.\nData Integrity and Immutability As we have seen, the transactions’s Merkle tree root and the previous block hash is included in all the chain’s blocks.\nImagine now that a malicious actor tries to change the n’th block of the chain by changing a transaction contained in it. Every transaction has a unique transaction hash, so, the malicious transaction will have a different transaction hash than the original:\nThis alteration will cause the intermediate nodes and the root to change, resulting in the block header and block hash changing as well. Since the (n+1)th block uses the previous block hash to calculate its own hash, and the previous hash has changed, new block hashes must be recalculated for all subsequent blocks.\nThis is a very complicated thing to accomplish for two reasons:\nComputing a single block hash takes around 10 minutes: Block hashes should have a certain number of zeros at the beginning. This is done by searching for a particular value for the nonce field in the header. If the Merkle root changes, there is a very, very low probability that recomputing the same block hash with the same nonce will be a hash that fulfills the condition of having a certain amount of zeros at the beginning, and, if this happened, there is an even lower probability that all the next blocks won’t need to change the nonce. You’ll need to overpower the network to convince honest nodes that the modified chain is the real one. This is known as the 51% attack. As a result, blocks in a blockchain are immutable.\nSimplified Payment Verification By using Merkle Trees, payment verifications can be made in a more efficient way. A verifier can run a node that does not save all the transactions, just the block headers. To verify that a payment was made, it needs to verify that the transaction that backs up that payment is contained in some block of the chain.\nImagine we need to verify that Tx 6 is present in a block. The first thing to do is to retrieve from a full node the block where the transaction is located and some specific nodes (marked in green), known as “Merkle path”1 from the Merkle tree. After that the verifier can compute the Merkle branch of Tx 6 (marked in blue) up to the root and validate that the transaction is included in the block n.\nThe whole verification process is very fast to compute and uses very little data, saves up disk space, and eliminates the need of transferring large amounts of data.\nNOTE: Ethereum widely uses a variation of the Merkle Tree known as Patricia Merkle Trie. Check this for more information.\nConclusion Although the Merkle tree is a simple data structure, its simplicity doesn\u0026rsquo;t make it any less powerful. Using simple yet clever solutions is one of the reasons that makes computer science a beautiful discipline.\nIf you ever find yourself in a situation where you need to verify data integrity or identify where data has changed, a Merkle tree is a good and efficient candidate that can help solve that problem.\nBlockchain space is full of these kinds of solutions and many more will come in the future, let’s not forget that we are at the beginning of this journey and new applications for this technology are yet to be discovered. There is space for introducing clever solutions in those yet-to-be-discovered new applications!\nThe Merkle path is the minimum number of nodes required to calculate the root hash.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://nicoan.net/posts/merkle_tree/","summary":"\u003cp\u003eThe bitcoin network has been running non-stop since 2009 and at the time of writing this blog post, the whole blockchain occupies \u003ca href=\"https://news.bitcoin.com/bitcoin-blockchain-surpasses-half-terabyte-amid-soaring-transactions-and-increased-block-capacity/\"\u003emore than 500GB\u003c/a\u003e. Verifying information in that huge amount of data is not an easy task. Luckily, as many other computer science problems, this can be solved by using the correct data structure. In our case, that data structure is the Merkle tree.\u003c/p\u003e\n\u003ch1 id=\"what-is-a-merkle-tree-aka-hash-tree\"\u003eWhat is a Merkle Tree (a.k.a Hash Tree)?\u003c/h1\u003e\n\u003cp\u003eAs the name suggests, the Merkle tree is a data structure in form of a tree (usually binary) where:\u003c/p\u003e","title":"Fast data verification in the blockchain: The Merkle Tree"},{"content":"Now it is time to talk about references and borrowing. To understand this topic, first check out this post where I talk about ownership and move semantics. As we have seen in the named article, the way Rust manages memory allocations is rather unique. This is also true when we talk about referencing some place in the memory, something that can be achieved in C with pointers.\nGDB In this post I am going to explore what is happening in memory using the GNU Debugger (gdb) with the special command rust-gdb:\n$ rust-gdb ./target/debug/references_and_borrowing What is a reference in Rust? A reference is a value that points to data in memory. Although it is similar to a classic pointer there is a crucial difference between the two: a reference is guaranteed to always point to a memory address that contains a valid piece of data whereas pointers are not¹. The checks performed to guarantee that a reference is always valid is done at compile time.\nConsider the following code:\n1 2 3 4 5 6 7 8 9 fn main() { // Create a new value, with s1 as owner let s1 = String::from(\u0026#34;hello world!\u0026#34;); // Create a reference of s1 let s2 = \u0026amp;s1; // Print the s2 value println!(\u0026#34;{}\u0026#34;, s2); } This code compiles and runs correctly. What happens in memory? Let’s check it out with GDB!\nBreakpoint 1, references_and_borrowing::main () at src/main.rs:3 3 let s1 = String::from(\u0026#34;hello world!\u0026#34;); (gdb) n 5 let s2 = \u0026amp;s1; (gdb) n 8 println!(\u0026#34;{}\u0026#34;, s2); At this point, the String s1 is initialized with the text \u0026quot;hello world! and the s1\u0026rsquo;s reference named s2 is set. Let\u0026rsquo;s check the stack:\n1 2 3 4 5 6 7 8 9 10 0x7fffffffd920: 56 217 255 255 255 127 0 0 0x7fffffffd928: 112 251 90 85 85 85 0 0 0x7fffffffd930: 32 208 255 247 255 127 0 0 0x7fffffffd938: 160 251 90 85 85 85 0 0 0x7fffffffd940: 12 0 0 0 0 0 0 0 0x7fffffffd948: 12 0 0 0 0 0 0 0 0x7fffffffd950: 56 217 255 255 255 127 0 0 0x7fffffffd958: 128 167 218 247 255 127 0 0 0x7fffffffd960: 0 0 0 0 0 0 0 0 0x7fffffffd968: 0 0 0 0 0 0 0 0 The lines 4 to 6 is the representation of s1 in the stack: 0x7fffffffd938 is ptr, 0x7fffffffd940 is len and 0x7fffffffd948 is capacity. The reference to s1 is located at 0x7fffffffd950. Let\u0026rsquo;s print the address value in hexadecimal:\n(gdb) x/xg 0x7fffffffd950 0x7fffffffd950: 0x00007fffffffd938 As we can see, the value contained in the address 0x7fffffffd950 is 0x00007fffffffd938², the beginning of the s1\u0026rsquo;s stack representation!\n¹ An invalid memory region refers to a region that was not assigned to our process or memory that was valid at some point of the program execution but then was freed. ² Zeroes are trimmed for legibility when printed as a memory address by GDB.\nThe two rules of references As everything in Rust, references have their own set of rules.\nReferences are always valid. At any given time we either have any number of immutable references or one mutable reference. References are always valid There\u0026rsquo;s no way of testing this rule at runtime (or at least I don\u0026rsquo;t know one). As I stated earlier in this post, references are guaranteed to always be valid and this validation is done at compile time.\nAt any given time we either have any number of immutable references or one mutable reference At first glance, this rule feels like an unnecessary limitation but thanks to it we are able to catch hidden bugs in our code because data races are avoided at compile time.\nA classic example is the one where we have n mutable references of the same piece of numeric data that represents a counter, all in different threads. The only thing the threads do is increment the counter. References by themselves do not have a synchronization mechanism. This is the concurrent counter problem, here\u0026rsquo;s the whole explanation and an example code in Java. This can\u0026rsquo;t happen in Rust (code won\u0026rsquo;t compile) since we need some kind of synchronization mechanism to mutate the same piece of data in different threads.\nThis is not the only problem this rule keeps us away from! In fact, we don\u0026rsquo;t even need concurrency, it can avoid bugs in simpler situations. Consider the following code in python:\n1 2 3 4 5 6 7 8 def insert_even_zeros(vec): for (n,i) in enumerate(vec): if n % 2 == 0: vec.insert(i, 0) vec = list(range(1,7)) # [1, 2, 3, 4, 5, 6] print(vec) What we are trying to do here is to insert a 0 at the index of a value, if the value is an even number. The expected result for the input [1, 2, 3, 4, 5, 6] is [1, 0, 2, 3, 0, 4, 5, 0, 6] but if we run it, we get [1, 0, 0, 0, 0, 0, 0, 2, 3, 4, 5, 6]. What is happening? The source of the problem resides in the fact that we are mutating the vector while iterating it:\nWe start at index 0 where the value 1 is located, since 1 is not even we continue to index 1. At index 1 we find value 2. It is even so we insert a 0 at index 1. Now the array is: [1, 0, 2, 3, 4, 5, 6]. We continue to index 2. At index 2 we find the value 2 again, because it was moved from its original position in the previous iteration. It is even so we insert a 0 at index 2. Now the array is: [1, 0, 0, 2, 3, 4, 5, 6]. This process is repeated 4 more times since the array length is 6 and how many iterations are going to be executed is calculated at the beginning of the for statement. This is known as the iterator invalidation problem.\nWhat happens in Rust?\n1 2 3 4 5 6 7 8 9 10 11 12 fn insert_even_zeros(vec: \u0026amp;mut Vec\u0026lt;u8\u0026gt;) { for (i, n) in vec.iter().enumerate() { if n % 2 == 0 { vec.insert(i, 0); } } } fn main() { let mut v: Vec\u0026lt;u8\u0026gt; = (1..=6).collect(); // [1, 2, 3, 4, 5, 6] insert_even_zeros(\u0026amp;mut v); } We get a compilation error that enforces the rule!\nerror[E0502]: cannot borrow `*vec` as mutable because it is also borrowed as immutable --\u0026gt; src/main.rs:4:13 | 2 | for (i, n) in vec.iter().enumerate() { | ---------------------- | | | immutable borrow occurs here | immutable borrow later used here 3 | if n % 2 == 0 { 4 | vec.insert(i, 0); | ^^^^^^^^^^^^^^^^ mutable borrow occurs here For more information about this error, try `rustc --explain E0502`. Where are the mutable and immutable references? In the vector\u0026rsquo;s function signatures:\n.iter() takes an immutable reference of the vector (\u0026amp;self): pub fn iter(\u0026amp;self) -\u0026gt; Iter\u0026lt;\u0026#39;_, T\u0026gt; .insert() takes a mutable reference of the vector (\u0026amp;mut self): pub fn insert(\u0026amp;mut self, index: usize, element: T) Does this mean that there\u0026rsquo;s no way of modifying a vector in Rust while iterating it? No! You can do it:\n1 2 3 4 5 6 7 8 9 10 11 12 fn main() { let mut v: Vec\u0026lt;u8\u0026gt; = (1..=6).collect(); let mut i: usize = 0; let v_len = v.len(); while i \u0026lt; v_len { if v[i] % 2 == 0 { v.insert(i, 0); } i += 1 } } We also have two references, one immutable (len function) and one mutable (insert function). Why does it work? Because the scope of the immutable reference that is in len ends right after it the function is used (the scope of a reference begins at its creation and extends until the last time the reference is used).\nNotice that the error message we got with the for loop says \u0026ldquo;immutable borrow occurs here\u0026rdquo; and \u0026ldquo;immutable borrow later used here\u0026rdquo;. Both errors come from the same place, the iter() function, where the immutable reference is used.\nDoes it make sense for a programming language to have these kinds of rules if it is possible to write code to circumvent them? Yes! The way the last code is written is rather \u0026ldquo;unnatural\u0026rdquo;. Most of the time Rust will catch bugs at compile time thanks to these rules.\nBorrowing There are times when you don\u0026rsquo;t want a specific scope to lose ownership of a value. There could be several reasons for that, for example, you need to reuse the value. Consider the following code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 fn hello(s: String) { println!(\u0026#34;hello {s}.\u0026#34;); } fn bye(s: String) { println!(\u0026#34;bye {s}.\u0026#34;); } fn main() { let s = String::from(\u0026#34;fellow blog reader\u0026#34;); hello(s); bye(s); } This code won\u0026rsquo;t compile:\nerror[E0382]: use of moved value: `s` --\u0026gt; src/main.rs:12:9 | 10 | let s = String::from(\u0026#34;fellow blog reader\u0026#34;); | - move occurs because `s` has type `String`, which does not implement the `Copy` trait 11 | hello(s); | - value moved here 12 | bye(s); | ^ value used here after move | note: consider changing this parameter type in function `hello` to borrow instead if owning the value isn\u0026#39;t necessary --\u0026gt; src/main.rs:1:13 | 1 | fn hello(s: String) { | ----- ^^^^^^ this parameter takes ownership of the value | | | in this function help: consider cloning the value if the performance cost is acceptable | 11 | hello(s.clone()); | ++++++++ Here we have a similar situation as we had here. As the compiler error says, we are moving s into hello, so when we try to use it in bye we get the \u0026ldquo;use after move\u0026rdquo; error. How can we solve this?\nSolution 1: Duplicating the value We can do as the compiler says, and clone the value. This way, both functions get a separate copy of the value that they can own:\n1 2 3 4 5 6 7 8 9 10 11 12 13 fn hello(s: String) { println!(\u0026#34;hello {s}.\u0026#34;); } fn bye(s: String) { println!(\u0026#34;bye {s}.\u0026#34;); } fn main() { let s = String::from(\u0026#34;fellow blog reader\u0026#34;); hello(s.clone()); bye(s); } This works! the code compiles and executes without a warning. Is this a good solution? No.\nWe don\u0026rsquo;t really need to duplicate s since we are only reading it to print it out. This solution does a lot of extra work by duplicating s\u0026rsquo;s value in memory.\nSolution 2: Returning the ownership back to the caller Instead of duplicating s\u0026rsquo;s value, we can return the ownership to the caller, so it can use it again:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 fn hello(s: String) -\u0026gt; String { println!(\u0026#34;hello {s}.\u0026#34;); s } fn bye(s: String) { println!(\u0026#34;bye {s}.\u0026#34;); } fn main() { let s = String::from(\u0026#34;fellow blog reader\u0026#34;); let s2 = hello(s); bye(s2); } This also works! the code compiles and executes without a warning. Is this a good solution? Also no.\nPassing ownership back and forth functions is not a very comfortable and idiomatic way of doing things. On top of that, the function signatures are not semantically accurate. The signature of hello suggests that we pass an String value and returns back another String value. By just looking at it, it is hard to understand what the function intends to do and it does not make sense to return anything if the only objective of the function is only to print something.\nSolution 3: Borrowing We need to keep the ownership of s in the scope of the main function, we don\u0026rsquo;t want to duplicate values and we don\u0026rsquo;t want to move the values back and forth either. What can we do? use a reference!\n1 2 3 4 5 6 7 8 9 10 11 12 13 fn hello(s: \u0026amp;String) { println!(\u0026#34;hello {s}.\u0026#34;); } fn bye(s: \u0026amp;String) { println!(\u0026#34;bye {s}.\u0026#34;); } fn main() { let s = String::from(\u0026#34;fellow blog reader\u0026#34;); hello(\u0026amp;s); bye(\u0026amp;s); } The code compiles and executes without a warning. Is this a good solution? Yes.\nGiven that we only need to read the value, we don\u0026rsquo;t want to move it or duplicate it, using a reference is the best solution. Also, it is more idiomatic and semantically correct. By looking at the function\u0026rsquo;s signatures we know that they do not need to own any value and they will not return any result from the operation they are performing.\nLet\u0026rsquo;s now check what is happening in the memory with GDB:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Breakpoint 1, references_and_borrowing::main () at src/main.rs:10 10 let s = String::from(\u0026#34;fellow blog reader\u0026#34;); (gdb) n 11 hello(\u0026amp;s); (gdb) x/80ub $sp 0x7fffffffd970: 2 0 0 0 0 0 0 0 0x7fffffffd978: 128 217 255 255 255 127 0 0 0x7fffffffd980: 160 251 90 85 85 85 0 0 0x7fffffffd988: 18 0 0 0 0 0 0 0 0x7fffffffd990: 18 0 0 0 0 0 0 0 0x7fffffffd998: 48 251 90 85 85 85 0 0 0x7fffffffd9a0: 1 0 0 0 0 0 0 0 0x7fffffffd9a8: 59 216 85 85 85 85 0 0 0x7fffffffd9b0: 0 240 127 255 255 127 0 0 0x7fffffffd9b8: 48 251 90 85 85 85 0 0 Looks like our String representation in the stack starts at 0x7fffffffd980. Let\u0026rsquo;s confirm it.\n(gdb) x/xg 0x7fffffffd980 0x7fffffffd980: 0x00005555555afba0 (gdb) x/18c 0x00005555555afba0 0x5555555afba0: 102 \u0026#39;f\u0026#39; 101 \u0026#39;e\u0026#39; 108 \u0026#39;l\u0026#39; 108 \u0026#39;l\u0026#39; 111 \u0026#39;o\u0026#39; 119 \u0026#39;w\u0026#39; 32 \u0026#39; \u0026#39; 98 \u0026#39;b\u0026#39; 0x5555555afba8: 108 \u0026#39;l\u0026#39; 111 \u0026#39;o\u0026#39; 103 \u0026#39;g\u0026#39; 32 \u0026#39; \u0026#39; 114 \u0026#39;r\u0026#39; 101 \u0026#39;e\u0026#39; 97 \u0026#39;a\u0026#39; 100 \u0026#39;d\u0026#39; 0x5555555afbb0: 101 \u0026#39;e\u0026#39; 114 \u0026#39;r\u0026#39; Excellent, now let\u0026rsquo;s continue with the program execution and check what\u0026rsquo;s in hello function\u0026rsquo;s stack:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Breakpoint 2, references_and_borrowing::hello (s=0x7fffffffd980) at src/main.rs:2 2 println!(\u0026#34;hello {s}.\u0026#34;); (gdb) x/80ub $sp 0x7fffffffd900: 128 217 255 255 255 127 0 0 0x7fffffffd908: 140 201 85 85 85 85 0 0 0x7fffffffd910: 32 208 255 247 255 127 0 0 0x7fffffffd918: 128 217 255 255 255 127 0 0 0x7fffffffd920: 128 217 255 255 255 127 0 0 0x7fffffffd928: 154 177 222 247 255 127 0 0 0x7fffffffd930: 160 251 90 85 85 85 0 0 0x7fffffffd938: 18 0 0 0 0 0 0 0 0x7fffffffd940: 18 0 0 0 0 0 0 0 0x7fffffffd948: 213 192 89 85 85 85 0 0 (gdb) x/xg 0x7fffffffd920 0x7fffffffd920: 0x00007fffffffd980 At 0x7fffffffd920 found a pointer pointing to s in main\u0026rsquo;s stack (0x7fffffffd920: 0x00007fffffffd980)! We can confirm that the whole representation still belongs to main\u0026rsquo;s scope and, in hello and bye functions, we are just referencing it. smemory will be freed once main finishes.\nThere\u0026rsquo;s no need to change the scope to borrow a value: the code used in the previous section, is just a slight modification of an example used in a previous post that did not compile. We fixed it by borrowing s1\u0026rsquo;s value to s2.\nConclusion Sometimes we have a hard time fighting the Rust compiler because it usually fails with errors that do not exist in other programming languages. Those errors feel arbitrary but, as we have seen in this post, they are there to protect us. It can take some time to wrap your head around them.\nThe more you code in Rust, the less you fight with the compiler and you end up with more performant and more secure programs. Also, a lot of errors are caught at compile time, saving us a lot of precious debugging time.\nThis post concludes a series of post about how Rust handles memory the internals of it:\nStack and Heap Rust ownership and move semantics from the inside Rust references and borrowing from the inside ","permalink":"https://nicoan.net/posts/references_and_borrowing/","summary":"\u003cp\u003eNow it is time to talk about references and borrowing. To understand this topic, first check out this \u003ca href=\"/posts/move_semantics\"\u003epost\u003c/a\u003e where I talk about ownership and move semantics. As we have seen in the named article, the way Rust manages memory allocations is rather unique. This is also true when we talk about referencing some place in the memory, something that can be achieved in C with pointers.\u003c/p\u003e\n\u003ch1 id=\"gdb\"\u003eGDB\u003c/h1\u003e\n\u003cp\u003eIn this post I am going to explore what is happening in memory using the \u003ca href=\"https://en.wikipedia.org/wiki/GNU_Debugger\"\u003eGNU Debugger (gdb)\u003c/a\u003e with the special command \u003ccode\u003erust-gdb\u003c/code\u003e:\u003c/p\u003e","title":"Rust references and borrowing from the inside"},{"content":"Seed phrase and private key are two terms that are tightly related but are different things. When setting up a new crypto wallet such as Metamask, Ledger or Trezor, among others, a list of 12 or 24 common words is given to the user to back up. That list is the so-called seed phrase, an “easy to remember, easy to backup” word list.\nA private key is a large binary string used by a crypto wallet for signing the transactions that later will be sent to the chain.\nIn this article we are going to take a journey from the generation of the seed phrase to the multiple wallet address generation, making a special stop at the private key creation and paying attention to the technical details of this process.\nHow is the seed phrase generated? The BIP39 proposal BIP stands for Bitcoin Improvement Proposal. They are documents that describe a proposal for improving the Bitcoin blockchain ecosystem. BIP39 is called “Mnemonic code for generating deterministic keys” and describes the process of generating the mnemonic code. Here’s how it is done:\nThe words are taken from a word list that is available in several languages. The length of the list is always 2048 (2048 = 2¹¹, the number of bits used for grouping ENT + CS).\nThe quantity of words produced depends on the amount of entropy bits (ENT – the randomness factor of the initial seed). Typically the lengths used are 128 bits, leading to 12 words, and 256 bits, resulting in 24 words.\nHow is the private key generated? Binary seed Once the mnemonic seed phrase is obtained, numerous private keys can be generated from it. That is why wallets such as Metamask and Trezor are able to generate any number of accounts needed with only one seed phrase.\nTo generate the private keys from the seed phrase, the initial step involves computing a binary seed derived from the mnemonic. This computation uses the key stretching algorithm PBKDF2 with HMAC-SHA512, where the seed phrase is concatenated with the string “mnemonic” and a password utilized as a salt. In instances where no password is utilized, only the term “mnemonic” is appended.\nEmploying PBKDF2 increases the time and computational power needed for brute-force attacks, rendering it highly impractical to utilize this approach for cracking a seed phrase.\nIt is worth mentioning that this process and the seed phrase generation are completely separated processes. This process can be used with a mnemonic seed generated from a totally different process.\nPrivate keys Now it is time to use the binary seed to generate private keys (and from those, public keys and wallet addresses). The following process describes what it is known as a HD Wallet (Hierarchical Deterministic Wallet). First we need to create a Master Private Key, a Master Public Key and a Master Chain Code from the Binary seed:\nFrom this derivation we will produce all the accounts we need in a deterministic way. To create new accounts, we require:\nEither the Master Private Key or the Master Public Key. The Master Chain Code: this will be used as a source of entropy. A 32 bit integer number used as index. In the diagram above we used the Master Private Key, but as said earlier we can also use the Master Public Key. The difference between using one or the order is generating a Hardened Key or a Normal Key, but that is out of the scope of this article.\nWe can create a virtually limitless quantity of private keys with this method either by changing the index or using a Child Key as a Master Key and repeat the process. This way we can have a tree of private keys.\nGluing it all together We saw how a seed phrase is generated, used to generate a binary seed that then is used to generate a Master Private Key. This Master Private Key allows us to generate multiple Child Private Keys.\nPublic Keys and Wallet addresses are derived from Child Private Keys. It is a chain of derivation:\nDoes this mean that I always need a seed phrase to generate private keys? No! In fact, at the beginning of the blockchain era, all private keys were created individually.\nSecurity Is it worth trying to guess a seed phrase? Imagine that your seed phrase is composed of only one word of the word list. This means that the attacker has to try only 2048 combinations to derive your private key and steal all your assets. Suppose that deriving a private key with a specific combination takes 1 millisecond, then, it would take around 2 seconds to test all the possibilities!\nImagine now that your seed phrase is composed of two words of the word list. This means that the attacker has to try 2048 x 2048 = 2048² = 4194304 combinations to derive your private key. Assuming the 1-millisecond scenario, it would take the attacker around 1 hour and 10 minutes to test all the possibilities. That is a lot more!\nLet’s repeat the process but now with 12 words. This means the attacker has to try 2048¹² = 5444517870735015415413993718908291383296 combinations. It would take around 2048¹²÷1000÷60÷60÷24÷365 ≈ 172644529132896227023528466479 years.\nConclusion We saw the main difference between seed phrase vs private key, how it helps to keep your assets safe (by saving a phrase instead of a very long easy to forget number).\nAre we at the end of the road? It doesn’t seem to be the case, even though this is a huge improvement on how wallets are created, managed and recovered it is still not enough to enable massive adoption of the crypto ecosystem. There are currently a lot of efforts to make the crypto space more user friendly. A proof of that is Ethereum’s ERC-4337, known as “account abstraction” that will, among other things, allow the user to use standard methods of authentication such as fingerprint instead of having to store a mnemonic phrase.\nThe crypto space traveled a long distance to be where it is today, but there’s still a lot ahead that needs to be explored, new methods to develop and new things to be discovered. In my opinion, this is just the beginning of this amazing trip. Even though mnemonic phrases are the way-to-go today, it is worth paying attention to the new solutions that arise from the community. One or many of those new ideas will help to on-board the next generation of users and allow the mass adoption of this amazing technology.\n","permalink":"https://nicoan.net/posts/seed_phrase_vs_private_key/","summary":"\u003cp\u003eSeed phrase and private key are two terms that are tightly related but are different things. When setting up a new crypto wallet such as Metamask, Ledger or Trezor, among others, a list of 12 or 24 common words is given to the user to back up. That list is the so-called seed phrase, an “easy to remember, easy to backup” word list.\u003c/p\u003e\n\u003cp\u003eA private key is a large binary string used by a crypto wallet for signing the transactions that later will be sent to the chain.\u003c/p\u003e","title":"Seed Phrase vs Private Key: a technical overview"},{"content":"It’s common to hear about web3 initiatives incorporating or aiming to incorporate ZK-proofs for increased privacy. The notion of Zero Knowledge proof (ZK-proof) is not a modern concept; it was first introduced in a paper named “The Knowledge Complexity of Interactive Proof-Systems” in 1985. In this article we will see what a ZK-proof is, offer an intuitive explanation and explore several use cases.\nWhether you are a developer or a user, it’s important to understand the core concept behind ZK-Proofs because it will help you understand not only what the product is trying to achieve in matters of security and privacy but also how they are trying to achieve them.\nWhat is a Zero-Knowledge Proof? ZK-Proof is protocol that involves a Prover and a Verifier and enables the Prover to demonstrate the truth of a statement to the Verifier without disclosing any additional information beyond the veracity of the statement. Essentially, it allows the Prover to prove to the Verifier that they possess knowledge of a specific piece of information, without disclosing it.\nHow does it work? An intuitive explanation The Ali Baba cave is a well-known story used to give an intuitive explanation of how the ZK-Proof protocol works. This story is based in a circular cave with one entrance and a magic door opened by a password located in the middle of it. It involves two actors, Bob the verifier and Alice the Prover.\nAlice wants to prove to Bob that she knows the password that opens the door placed inside the cave without telling him the word itself. To do that they follows three simple steps:\nAlice chooses a random path to enter the cave while Bob waits outside without looking at her. Bob goes to the entrance of the cave and screams a path. Alice returns to the entrance using the path that Bob screamed in step 2. If Bob screams A, and Alice returns using A it can mean two things: either Alice entered the cave through B and knows the password or entered through A and cheated. After all, there is a 50% chance of getting it right.\nThe key is to iterate this process until it becomes highly improbable for Alice to have cheated in every repetition, given that each time Alice is correct, the probability of cheating is halved.\nWhere can we apply ZK-Proofs? Any scenario where someone needs to demonstrate the validity of some statement without giving up the information that validates it is suitable. In web3, ZK-Proofs can enhance privacy. For instance, they can be used to anonymize transactions between peers, as done in the ZCash z-address to z-address transactions, or to enable anonymous voting for decentralized autonomous organizations (DAOs).\nGiven the fact that the ZK-Profs are a general cryptography concept, applications can also be found for web2: financial institutions could leverage ZK-Proofs to demonstrate that a person’s income falls within a specific range, without disclosing the precise income amount or or as a proof of identity, demonstrating that the possession a valid government issued document, without giving it up.\nZK-Proofs are perfect for this kind of scenario because they can provide strong privacy guarantees without compromising the integrity of the data managed by the system.\nConclusion Zero-Knowledge proofs have the potential to make the web3 space a more private and safer environment for the users. But as Uncle Ben said in the best Spider-Man movie: “with great power comes great responsibility”. We can’t ignore the fact that these tools can also be used by bad actors to perform illicit activities such as money laundering and illegal transactions. As we continue to develop and apply this technology, it is crucial to consider methods for combating its misuse without sacrificing its benefits. As web3 enthusiasts and developers this can’t be ignored and should be discussed along with the development of ZK-Proof based solutions.\n","permalink":"https://nicoan.net/posts/zero_knowledge_proofs_decoded/","summary":"\u003cp\u003eIt’s common to hear about web3 initiatives incorporating or aiming to incorporate ZK-proofs for increased privacy. The notion of Zero Knowledge proof (ZK-proof) is not a modern concept; it was first introduced in a paper named \u003ca href=\"http://people.csail.mit.edu/silvio/Selected%20Scientific%20Papers/Proof%20Systems/The_Knowledge_Complexity_Of_Interactive_Proof_Systems.pdf\"\u003e“The Knowledge Complexity of Interactive Proof-Systems”\u003c/a\u003e in 1985. In this article we will see what a ZK-proof is, offer an intuitive explanation and explore several use cases.\u003c/p\u003e\n\u003cp\u003eWhether you are a developer or a user, it’s important to understand the core concept behind ZK-Proofs because it will help you understand not only what the product is trying to achieve in matters of security and privacy but also how they are trying to achieve them.\u003c/p\u003e","title":"Zero-Knowledge Proofs Decoded: A Simple Intro"},{"content":"Ownership and move semantics is one of the things that makes Rust unique. To understand this topic, you need to understand what Stack and Heap are at a basic level. I wrote a post about that! You can check it out if you need a refresher on those concepts. It is a little bit hard to get used to this feature because it forces you to think about stuff that you didn\u0026rsquo;t have to worry about in other languages. Enough introduction, let\u0026rsquo;s cut to the chase!\nGDB In this post, I am going to explore what is happening in memory using the GNU Debugger (gdb) with the special command rust-gdb:\n$ rust-gdb ./target/debug/move_semantics I am going to use the x command a lot to explore the stack and the $sp value (refers to the Stack Pointer).\nThe three rules of ownership There are three rules that governs the ownership system:\nEvery initialized value has an owner: Every initialized value has a variable that is its owner.¹ There is only one owner per value: You can\u0026rsquo;t have two or more variables that owns the same value in memory. You can\u0026rsquo;t share ownership between variables.² If a variable\u0026rsquo;s scope ends, its value gets freed: When a scope ends, all values owned by variables contained in that scope get automatically freed. ¹ But not every variable owns a value, they may just hold a reference. I\u0026rsquo;ll talk about this in the \u0026ldquo;References and Borrowing\u0026rdquo; article. ² Actually you can have more than one owner in safe Rust. You have to use special structures, such as Rc (multiple owners do not own the value directly though).\nLet\u0026rsquo;s test the rules! But before that, a little reminder of how the String type is represented in memory:\nwhere:\nptr: A pointer to the first direction of the Heap containing the string itself (in this case hello). len: How much memory, in bytes, the contents of the string is currently using. capacity: The total amount of memory, in bytes, allocated for that string. Rule 1: Every initialized value has an owner. Consider the following code:\n1 2 3 4 5 6 7 8 9 fn hello_world() -\u0026gt; u32 { String::from(\u0026#34;hello! I am a free initialized String!\u0026#34;); println!(\u0026#34;{}\u0026#34;, 42); 42 } fn main() { hello_world(); } In the hello_world function, we have an initialized String value that is free (not assigned to a variable). Did Rust initialize the value in memory or just ignore it? We can\u0026rsquo;t use it so\u0026hellip; Why would Rust save it? Let\u0026rsquo;s check what happens! When we compile this code we get the following warning:\nwarning: unused return value of `from` that must be used --\u0026gt; src/main.rs:2:5 | 2 | String::from(\u0026#34;hello! I am a free initialized String!\u0026#34;); | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ | = note: `#[warn(unused_must_use)]` on by default Rust warns us that we must use the returned value of the String::from function, otherwise, we can\u0026rsquo;t access it in any way. What happens in memory? Let\u0026rsquo;s check it out with GDB!\nFirst, we set a breakpoint at the beginning of the hello_world function and execute the String initialization:\nBreakpoint 1, move_semantics::hello_world () at src/main.rs:2 2 String::from(\u0026#34;hello! I am a free initialized String!\u0026#34;); (gdb) n 3 println!(\u0026#34;{}\u0026#34;, 42); At this point, the String is initialized, but it isn\u0026rsquo;t assigned to a variable. So, it has no owner! Let\u0026rsquo;s check the stack:\n1 2 3 4 5 6 7 8 9 10 11 (gdb) x/80ub $sp 0x7fffffffd980: 0 240 127 255 255 127 0 0 0x7fffffffd988: 61 60 87 85 85 85 0 0 0x7fffffffd990: 16 90 90 85 85 85 0 0 0x7fffffffd998: 38 0 0 0 0 0 0 0 0x7fffffffd9a0: 38 0 0 0 0 0 0 0 0x7fffffffd9a8: 2 0 0 0 0 0 0 0 0x7fffffffd9b0: 48 0 0 0 0 0 0 0 0x7fffffffd9b8: 96 255 255 255 255 255 255 255 0x7fffffffd9c0: 0 240 127 255 255 127 0 0 0x7fffffffd9c8: 5 0 0 0 0 0 0 0 It seems that the String value is there, lines 4 to 6 looks like our initialized value: memory addresses 0x7fffffffd998 and 0x7fffffffd9a0 (lines 5 and 6) have a 38 stored, and the string happens to have 38 characters. 0x7fffffffd990 (line 4) must be the Heap address where the actual text is allocated! Let\u0026rsquo;s see what\u0026rsquo;s inside that memory address.\nFirst, print the address as hexa:\n(gdb) x/xg 0x7fffffffd990 0x7fffffffd990:\t0x00005555555a5a10 Then, explore what\u0026rsquo;s inside that address!\n1 2 3 4 5 6 (gdb) x/38cb 0x00005555555a5a10 0x5555555a5a10: 0 \u0026#39;\\000\u0026#39; 0 \u0026#39;\\000\u0026#39; 0 \u0026#39;\\000\u0026#39; 0 \u0026#39;\\000\u0026#39; 0 \u0026#39;\\000\u0026#39; 0 \u0026#39;\\000\u0026#39; 0 \u0026#39;\\000\u0026#39; 0 \u0026#39;\\000\u0026#39; 0x5555555a5a18: 16 \u0026#39;\\020\u0026#39; 80 \u0026#39;P\u0026#39; 90 \u0026#39;Z\u0026#39; 85 \u0026#39;U\u0026#39; 85 \u0026#39;U\u0026#39; 85 \u0026#39;U\u0026#39; 0 \u0026#39;\\000\u0026#39; 0 \u0026#39;\\000\u0026#39; 0x5555555a5a20: 101 \u0026#39;e\u0026#39; 101 \u0026#39;e\u0026#39; 32 \u0026#39; \u0026#39; 105 \u0026#39;i\u0026#39; 110 \u0026#39;n\u0026#39; 105 \u0026#39;i\u0026#39; 116 \u0026#39;t\u0026#39; 105 \u0026#39;i\u0026#39; 0x5555555a5a28: 97 \u0026#39;a\u0026#39; 108 \u0026#39;l\u0026#39; 105 \u0026#39;i\u0026#39; 122 \u0026#39;z\u0026#39; 101 \u0026#39;e\u0026#39; 100 \u0026#39;d\u0026#39; 32 \u0026#39; \u0026#39; 83 \u0026#39;S\u0026#39; 0x5555555a5a30: 116 \u0026#39;t\u0026#39; 114 \u0026#39;r\u0026#39; 105 \u0026#39;i\u0026#39; 110 \u0026#39;n\u0026#39; 103 \u0026#39;g\u0026#39; 33 \u0026#39;!\u0026#39; Our String is mostly there! But, it appears that the beginning of it was overwritten. It\u0026rsquo;s ok, that value isn\u0026rsquo;t owned by any variable; we can\u0026rsquo;t access it. So, it doesn\u0026rsquo;t matter what happens to it.\nNOTE: This memory exploration was done using a debug build. I am not really sure what happens if this code was compiled in release mode. I believe that Rust does not initialize the value as an optimization, because it is not used.\nRule 2: There\u0026rsquo;s only one owner per value Consider the following code:\n1 2 3 4 5 6 7 8 fn main() { // Create a new value, with s1 as owner let s1 = String::from(\u0026#34;hello world!\u0026#34;); // Move ownership from s1 to s2 let s2 = s1; // Oops! compiler error, the value has been moved! println!(\u0026#34;{}\u0026#34;, s1); } When we try to compile this, we get:\nerror[E0382]: borrow of moved value: `s1` --\u0026gt; src/main.rs:7:20 | 3 | let s1 = String::from(\u0026#34;hello world!\u0026#34;); | -- move occurs because `s1` has type `String`, which does not implement the `Copy` trait 4 | // Move ownership from s1 to s2 5 | let s2 = s1; | -- value moved here 6 | // Oops! compiler error, the value has been moved! 7 | println!(\u0026#34;{}\u0026#34;, s1); | ^^ value borrowed here after move What is happening here is that the ownership of the String \u0026quot;hello world!\u0026quot; is transferred from s1 to s2. Because of that, the compiler invalidates the access to s1.\nThe value was moved because the type String does not implement the Copy trait. This is used on types that can be fully allocated in the stack and can be duplicated by simply copying bits without much overload (duplicating data in the Heap is much more complicated). When a type implements the Copy trait, instead of having \u0026ldquo;move semantics\u0026rdquo; it has \u0026ldquo;copy semantics\u0026rdquo;. This is usually the case for primitive types:\n1 2 3 4 5 6 fn main() { let n1 = 42; let n2 = n1; println!(\u0026#34;{}\u0026#34;, n1); println!(\u0026#34;{} {}\u0026#34;, n1, n2); } If we run this code\u0026hellip;\ncargo run Compiling move_semantics v0.1.0 (/home/rust/blog) Finished dev [unoptimized + debuginfo] target(s) in 0.30s Running `target/debug/move_semantics` 42 42 42 compiles! Because the value 42 is copied!\nRule 3: If a variable\u0026rsquo;s scope ends its value gets freed Consider the following code:\n1 2 3 4 5 6 7 8 fn main() { { // Create a new value with s1 as owner let s1 = String::from(\u0026#34;hello world!\u0026#34;); } // s1 gets dropped here! since is the end of the scope println!(\u0026#34;Checking drop with gdb!\u0026#34;); } s1 allocation will have been freed when we reach line 7. This is because the curly braces at the beginning of the main function creates a new scope. Once the code reaches the end of it, all the variables that it contained get dropped. Let\u0026rsquo;s check it out in GDB:\nOn line 4, we can find s1 in the locals variables of the scope:\nBreakpoint 1, move_semantics::main () at src/main.rs:4 4\tprintln!(\u0026#34;{}\u0026#34;, s1); (gdb) info locals s1 = \u0026#34;hello world!\u0026#34; Let\u0026rsquo;s check where the Heap allocation of s1 is and what value it contains (remember that the first field of the Stack representation is the pointer to the Heap):\n(gdb) p \u0026amp;s1 $1 = (*mut alloc::string::String) 0x7fffffffd960 (gdb) x/xg 0x7fffffffd960 0x7fffffffd960:\t0x00005555555a5ad0 (gdb) x/12c 0x00005555555a5ad0 0x5555555a5ad0:\t104 \u0026#39;h\u0026#39;\t101 \u0026#39;e\u0026#39;\t108 \u0026#39;l\u0026#39;\t108 \u0026#39;l\u0026#39;\t111 \u0026#39;o\u0026#39;\t32 \u0026#39; \u0026#39;\t119 \u0026#39;w\u0026#39;\t111 \u0026#39;o\u0026#39; 0x5555555a5ad8:\t114 \u0026#39;r\u0026#39;\t108 \u0026#39;l\u0026#39;\t100 \u0026#39;d\u0026#39;\t33 \u0026#39;!\u0026#39; But when the scope finishes\u0026hellip;\n7\tprintln!(\u0026#34;Checking drop with gdb!\u0026#34;); (gdb) info locals No locals. (gdb) x/12c 0x00005555555a5ad0 0x5555555a5ad0:\t0 \u0026#39;\\000\u0026#39;\t0 \u0026#39;\\000\u0026#39;\t0 \u0026#39;\\000\u0026#39;\t0 \u0026#39;\\000\u0026#39;\t0 \u0026#39;\\000\u0026#39;\t0 \u0026#39;\\000\u0026#39;\t0 \u0026#39;\\000\u0026#39;\t0 \u0026#39;\\000\u0026#39; 0x5555555a5ad8:\t16 \u0026#39;\\020\u0026#39;\t80 \u0026#39;P\u0026#39;\t90 \u0026#39;Z\u0026#39;\t85 \u0026#39;U\u0026#39; All the locals variables were dropped and the memory occupied by them freed! That part of the Heap is now filled with something else (probably garbage).\nMoving a value: What happens under the hood? Consider the following code:\n1 2 3 4 5 6 7 8 9 10 11 12 fn move_stack_example() { // Create a new String value let s1 = String::from(\u0026#34;hello world!\u0026#34;); // Move it from s1 to s2 (s2 takes ownership) let s2 = s1; println!(\u0026#34;{}\u0026#34;, s2); } fn main() { move_stack_example(); } When we move a value, it does not dissapears from the memory, instead, whatever is in the stack that belongs to the moved value gets duplicated and the compiler just forbids us from accessing the old variable ever again.\nLet\u0026rsquo;s verify what I just said with GDB. We are going to examine the stack frame of the move_stack_example function. First of all, let\u0026rsquo;s check the locals variables:\n(gdb) info locals s2 = \u0026#34;hello world!\u0026#34; s1 = \u0026#34;hello world!\u0026#34; Whoa! Looks like s1 and s2 have the same value! Actually they are pointing to the same value. Let\u0026rsquo;s now see what the addresses of s1 and s2 are:\n(gdb) p \u0026amp;s1 $1 = (*mut alloc::string::String) 0x7fffffffdb08 (gdb) p \u0026amp;s2 $2 = (*mut alloc::string::String) 0x7fffffffdb20 Great! Now, we know that s1\u0026rsquo;s stack representation starts at 0x7fffffffdb08 and s2\u0026rsquo;s starts at 0x7fffffffdb20. Let\u0026rsquo;s now see the contents of the stack frame:\n1 2 3 4 5 6 7 8 9 10 11 (gdb) x/80bu $sp 0x7fffffffdaf0:\t112\t171\t217\t247\t255\t127\t0\t0 0x7fffffffdaf8:\t7\t125\t221\t247\t255\t127\t0\t0 0x7fffffffdb00:\t2\t0\t0\t0\t0\t0\t0\t0 0x7fffffffdb08:\t208\t90\t90\t85\t85\t85\t0\t0 0x7fffffffdb10:\t12\t0\t0\t0\t0\t0\t0\t0 0x7fffffffdb18:\t12\t0\t0\t0\t0\t0\t0\t0 0x7fffffffdb20:\t208\t90\t90\t85\t85\t85\t0\t0 0x7fffffffdb28:\t12\t0\t0\t0\t0\t0\t0\t0 0x7fffffffdb30:\t12\t0\t0\t0\t0\t0\t0\t0 0x7fffffffdb38:\t0\t0\t0\t0\t0\t0\t0\t0 What do we have at s1 and s2 addresses? Let\u0026rsquo;s check it out!:\nptr: For s1 this value is at 0x7fffffffdb08. For s2 this value is at 0x7fffffffdb20. len: For s1 this value is at 0x7fffffffdb10. For s2 this value is at 0x7fffffffdb28. capacity: For s1 this value is at 0x7fffffffdb18. For s2 this value is at 0x7fffffffdb30. As you can see, both ptr values are the same, meaning that both variables are pointing to the same data in the Heap. Let\u0026rsquo;s print them in hexadecimal to get the correct format to explore it:\n(gdb) x/xg 0x7fffffffdb08 0x7fffffffdb08: 0x00005555555a5ad0 (gdb) x/xg 0x7fffffffdb20 0x7fffffffdb20: 0x00005555555a5ad0 So, the ptr value is 0x00005555555a5ad0! Now, take a look at the contents of that address in the Heap:\n(gdb) x/12c 0x00005555555a5ad0 0x5555555a5ad0:\t104 \u0026#39;h\u0026#39;\t101 \u0026#39;e\u0026#39;\t108 \u0026#39;l\u0026#39;\t108 \u0026#39;l\u0026#39;\t111 \u0026#39;o\u0026#39;\t32 \u0026#39; \u0026#39;\t119 \u0026#39;w\u0026#39;\t111 \u0026#39;o\u0026#39; 0x5555555a5ad8:\t114 \u0026#39;r\u0026#39;\t108 \u0026#39;l\u0026#39;\t100 \u0026#39;d\u0026#39;\t33 \u0026#39;!\u0026#39; The hello world! string is there!\nConclusion It can take some time to get used to working with ownership and move semantics, but, in my opinion, that is well invested time. Manually managing memory (by allocating and freeing it) is not an easy task and can create several bugs. With Rust\u0026rsquo;s approach, those bugs are caught at compile time, so they can never happen!\nIf you want to read more about this topic, check out the Rust book.\n","permalink":"https://nicoan.net/posts/move_semantics/","summary":"\u003cp\u003eOwnership and move semantics is one of the things that makes Rust unique. To understand this topic, you need to understand what Stack and Heap are at a basic level. I wrote a \u003ca href=\"/posts/stack_and_heap\"\u003epost\u003c/a\u003e about that! You can check it out if you need a refresher on those concepts. It is a little bit hard to get used to this feature because it forces you to think about stuff that you didn\u0026rsquo;t have to worry about in other languages. Enough introduction, let\u0026rsquo;s cut to the chase!\u003c/p\u003e","title":"Rust ownership and move semantics from the inside"},{"content":"Why write about Stack and Heap when there are already a lot of articles out there? I want to improve my writing skills so, I decided to write articles about things I find interesting. This article was supposed to be about how Rust manages memory through ownership. Then I thought \u0026ldquo;I should first write about Stack and Heap\u0026rdquo;. So, here we are).\nTo understand memory management first, we need to understand what the Stack and the Heap are. Stack and Heap are memory regions used by a process to store and read values. The memory of a running process can usually be divided in the following four regions:\nText: Here is where our program instructions live. Our compiled program is loaded and stored in this region of the memory. Data: All the global variables are stored in this region. Stack: A contiguous chunk of memory that stores local variables, arguments and return addresses of functions (we will go deeper on this in the next section). Every process\u0026rsquo; thread has its own Stack. Heap: Stores all the dynamically allocated memory. This region is shared among all threads of a process. Stack A process\u0026rsquo; Stack is an actual implementation of the Stack data structure. It is fixed in size; we can not ask the operating system for more memory. This size depends mostly on the OS. In modern Linux systems, the maximum Stack size is 8 MB (you can check yours with the command ulimit -s).\nInside the Stack Every time we call a function, a Stack frame (a chunk of contiguous memory containing all the information required by the recently called function) is created and placed on top of the Stack. And, every time a function ends, the Stack frame is popped from the Stack, automatically releasing all the memory used by it. Let\u0026rsquo;s see a minimal example on how the Stack is populated. Consider the following code:\nfn sum(a: i32, b: i32) -\u0026gt; i32 { let result = a + b; result } fn square_sum(a: i32, b: i32) -\u0026gt; i32 { let sum_result = sum(a, b); let pow_result = sum_result * sum_result; pow_result } fn main() { let n1 = 2; let n2 = 5; let pow_result = square_sum(n1, n2); println!(\u0026#34;Result: {}\u0026#34;, pow_result); } The following diagram represents what happens with the Stack when the program is executed:\nAt the beginning, a Stack frame for the main function is created. main calls square_sum, a Stack frame for square_sum is created. square_sum calls sum, a Stack frame is created for sum. After sum is called, its Stack frame is destroyed, releasing automatically all the memory it occupied. After square_sum is called, its Stack frame is destroyed, releasing automatically all the memory it occupied. main prints the result and ends. The operating system frees up all the remaining memory. NOTE: Usually, the stack grow downwards!\nInside the Stack frame The Stack frame is where all the local variables, arguments, and return address (this is used by the running process to know where the next code instruction to be executed is, after the function call ends) of a function live. The good news is that the user does not have to worry about allocating or de-allocating the memory used by it (neither with Heap allocations in safe Rust, but that is for another post). Given that the Stack is fixed in size, we can only store data that its size is known at compile time. For dynamic sized data (such as vectors), Heap memory is used (only a pointer to that part of the Heap memory and maybe some metadata is saved into the Stack). Let\u0026rsquo;s expand the above Stack diagrams to show the stack frame of each function:\nHow do we use the memory contained in the frame? We have two pointers that helps us with that:\nStack Pointer (SP): Always points at the top of the Stack. When a Stack frame is created the new SP\u0026rsquo;s value is SP + size_of(new_Stack_frame). It will change any time a value is pushed onto or popped off the stack. When an executing function returns, it goes back to its previous size. Base Pointer (BP): Also known as Frame Pointer (FP). Points to the base of the current Stack frame. When a Stack frame is created, the BP gets the value SP had before adding the size of the new Stack frame. The BP is used to access the arguments and local variables of a function by adding/substracting the offset of the variable we want to access. For example, if we want to access the argument y, we need to read the address BP + 12 because, first, we have the return address that (let\u0026rsquo;s assume) is 4 bytes long, and then the argument x that is a 32 bits integer (4 bytes). The layout presented here is just an example. Although, in reality, the frames contain the same information, how the data is organized depends on the machine architecture and the application binary interface (ABI).\nThis StackOverflow answer shows how the frame is constructed using x86 assembly.\nHeap In this context, \u0026ldquo;Heap\u0026rdquo; has nothing to do with the Heap data structure, it is just a name for the free memory pool.\nThe Heap is not fixed in size. We can ask for more memory, as long as it is available in the system, and free it if the allocated values are not needed anymore. Unlike Stack, when we are working with the Heap we have to take care of the allocation and deallocation of memory (in Rust, most allocation/deallocation logic is hidden behind abstractions). When we use the Heap, we are dynamically allocating memory. This comes in very handy when we are dealing with data which size is unknown at compile time (i.e. user input). In opposition to the Stack, the memory allocations are not sequential.\nWhen a process wants to allocate some chunk of memory of a given size, the operating system first has to search for a free piece of memory with the needed size. After finding it, the OS locks it up (only that particular process can access that portion of the system memory) and returns the starting address of the block. This process leads to memory fragmentation (the data allocated in the Heap is not contiguous).\nWhen we use the Heap, we also store some data in the Stack (at least a pointer to the allocated data). Consider the following code:\nfn main() { let n1: Box\u0026lt;u8\u0026gt; = Box::new(42); let my_string = String::from(\u0026#34;hello\u0026#34;); println!(\u0026#34;{} {}\u0026#34;, my_string, n1); } NOTE: A Box is a smart pointer to a heap allocated value.\nThe following diagram shows the allocations made in the Stack and the Heap:\nWriting and reading the Heap is slower than writing and reading the Stack for several reasons:\nFor memory allocation, a process has to make a system call and wait for the OS to complete the process described above. Using the allocated memory (for writing or reading) involves at least one indirection (following the pointer allocated in the Stack). Under the right conditions, a program can be optimized to store some parts of the Stack inside the processor\u0026rsquo;s cache, making writing/reading operations blazingly fast. Summary Stack Heap Fixed in size Can grow or shrink Allocations are in a contiguous block Allocation happens in \u0026ldquo;random\u0026rdquo; order Faster access time Slower access time Is thread local by default: every thread of a process has its own Stack. Can be used to share memory across threads ","permalink":"https://nicoan.net/posts/stack_and_heap/","summary":"\u003cp\u003eWhy write about Stack and Heap when there are already a lot of articles out there? I want to improve my writing skills so, I decided to write articles about things I find interesting. This article was supposed to be about how Rust manages memory through \u003ca href=\"https://doc.rust-lang.org/book/ch04-00-understanding-ownership.html\"\u003eownership\u003c/a\u003e. Then I thought \u0026ldquo;I should first write about Stack and Heap\u0026rdquo;. So, here we are).\u003c/p\u003e\n\u003cp\u003eTo understand memory management first, we need to understand what the Stack and the Heap are. Stack and Heap are memory regions used by a process to store and read values. The memory of a running process can usually be divided in the following four regions:\u003c/p\u003e","title":"Stack and Heap"}]